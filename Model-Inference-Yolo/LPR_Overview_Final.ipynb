{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ts664t76QNMq"
      },
      "source": [
        "# <font color='red' size='5px'/> LPR Project<font/>"
      ],
      "id": "Ts664t76QNMq"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "94xxko43WSsh"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Image"
      ],
      "id": "94xxko43WSsh"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G0rmTZvxQNNH"
      },
      "source": [
        "#<font color='blue' size='5px'/> Overview<font/>"
      ],
      "id": "G0rmTZvxQNNH"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xB4wLnm0QNNJ"
      },
      "source": [
        "## 1 Problem Statement"
      ],
      "id": "xB4wLnm0QNNJ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "**Problem Statement:**\n",
        "\n",
        "Design and implement a real-time license plate detection system capable of accurately detecting and localizing license plates in images. The system should be able to handle various environmental conditions, such as different lighting conditions, vehicle orientations, and background clutter, and provide reliable results for further processing or use in applications like traffic monitoring, parking management, or law enforcement.\n",
        "\n"
      ],
      "metadata": {
        "id": "2xacVIZuifxR"
      },
      "id": "2xacVIZuifxR"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQp9IYqVQNNO"
      },
      "source": [
        "## 2 Project Goal"
      ],
      "id": "BQp9IYqVQNNO"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Key Objectives and Requirements:**\n",
        "1. Develop or adopt a deep learning-based object detection model, such as YOLO, to detect license plates within a given image or video frame.\n",
        "2. Annotate and curate a dataset of images containing vehicles with license plates, including various scenarios and license plate types.\n",
        "3. Train the detection model on the dataset to achieve high accuracy and robustness in real-world conditions.\n",
        "4. Implement post-processing techniques, like Non-Maximum Suppression (NMS), to refine the detected license plate bounding boxes.\n",
        "\n",
        "6. Provide visual feedback by overlaying bounding boxes and text labels (license plate numbers) on detected license plates.\n",
        "7. Develop a user-friendly interface for testing the system on live camera feeds and pre-recorded video.\n",
        "8. Evaluate the system's performance using relevant metrics (e.g., precision, recall, F1-score) and ensure it meets or exceeds predefined accuracy targets.\n",
        "\n",
        "10. Document the system's architecture, training process, and deployment instructions for future maintenance and scalability.\n",
        "\n",
        "\n",
        "**Deliverables:**\n",
        "- Trained license plate detection model.\n",
        "- Software application or library for license plate detection with a user interface.\n",
        "- Documentation detailing system architecture, data preparation, model training, and deployment instructions.\n",
        "- Performance evaluation report, including accuracy metrics and real-time frame rates.\n",
        "\n"
      ],
      "metadata": {
        "id": "R5PrVsc3iriC"
      },
      "id": "R5PrVsc3iriC"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9iG6XT_3QNNY"
      },
      "source": [
        "# <font color='blue' size='5px'/> Literature Review<font/>"
      ],
      "id": "9iG6XT_3QNNY"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1 Introduction"
      ],
      "metadata": {
        "id": "c11pDtWHf5SW"
      },
      "id": "c11pDtWHf5SW"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Introduction**\n",
        "\n",
        "The video is about using YOLO for drowsiness detection, including leveraging the ultralytics of YOLO, fine-tuning the drowsiness model, and testing it in real-time. The presenter showcases the implementation of the YOLO model and performs real-time detections using images, videos, and a webcam. Additionally, the video covers training a custom model for drowsiness detection.\n",
        "\n",
        "**Loading Pre-Trained Ultralytics Model**\n",
        "\n",
        "To load the pre-trained ultralytics model, we need to import torch, matplotlib, numpy and cv2 libraries. The model detects 38 cars and 4 trucks with reasonable confidence intervals in a new image passed through a new link.\n",
        "\n",
        "**Real-Time Detection**\n",
        "\n",
        "The speaker demonstrates real-time detection using YOLO on a video and a webcam. The code is creating a full file path to the image with a unique identifier and .jpg file extension. The presenter also shows how to run YOLO on a video file.\n",
        "\n",
        "**Training Custom Drowsiness Detector Model**\n",
        "\n",
        "The speaker closed the real-time detection and will now train a custom drowsiness detector model using collected images and labels. Two key dependencies need to be installed, piqt5 and lxml, followed by running pi rcc5 command to seed the label image file. The trainer explains the training command and its parameters for YOLOv5 object detection.\n",
        "\n",
        "**Testing Custom Model**\n",
        "\n",
        "The YOLO model has finished training and generated data, now it's time to test it. The presenter shows how to test the custom model on a new image, with the output showing whether the person in the image is alert or drowsy.\n",
        "\n",
        "In summary, the video covers using YOLO for drowsiness detection, including loading pre-trained models, real-time detection on videos and webcams, training custom models, and testing them on new images."
      ],
      "metadata": {
        "id": "0fRZtRlsx592"
      },
      "id": "0fRZtRlsx592"
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Original Video](https://www.youtube.com/watch?v=tFNJGim3FXw)"
      ],
      "metadata": {
        "id": "h0G1-hijyBRv"
      },
      "id": "h0G1-hijyBRv"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2 Dataset"
      ],
      "metadata": {
        "id": "ywYGOoMAf8E8"
      },
      "id": "ywYGOoMAf8E8"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-WAV4YwCALat"
      },
      "source": [
        "#<font color='blue' size='5px'/> YOLOv5s Project<font/>"
      ],
      "id": "-WAV4YwCALat"
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Packages"
      ],
      "metadata": {
        "id": "lInTa0NEfAXq"
      },
      "id": "lInTa0NEfAXq"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "import torch\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "BsZ4vdf4jLQy"
      },
      "id": "BsZ4vdf4jLQy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n"
      ],
      "metadata": {
        "id": "hRyuV9-tDtRx"
      },
      "id": "hRyuV9-tDtRx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os"
      ],
      "metadata": {
        "id": "zWrmTzm0tkp3"
      },
      "id": "zWrmTzm0tkp3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fY2uss0opd1a",
        "outputId": "6aef6bc4-fe56-4754-d682-f6a98144011b"
      },
      "id": "fY2uss0opd1a",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms ## For Transformation on Images\n",
        "from torch.utils.data import DataLoader, Dataset"
      ],
      "metadata": {
        "id": "okEUNbrLptFZ"
      },
      "id": "okEUNbrLptFZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import  random_split"
      ],
      "metadata": {
        "id": "95dhNpxaudl5"
      },
      "id": "95dhNpxaudl5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Model"
      ],
      "metadata": {
        "id": "biXtL1CP-Jxp"
      },
      "id": "biXtL1CP-Jxp"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ultralytics"
      ],
      "metadata": {
        "id": "EZac0kzJOjoU"
      },
      "id": "EZac0kzJOjoU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ultralytics/yolov5  # clone\n",
        "!cd yolov5\n",
        "!pip install -r requirements.txt  # install"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oh3E_VKTOpZ1",
        "outputId": "5ea2b7f3-1cca-447d-c640-7dd35c49cc51"
      },
      "id": "Oh3E_VKTOpZ1",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'yolov5'...\n",
            "remote: Enumerating objects: 15967, done.\u001b[K\n",
            "remote: Total 15967 (delta 0), reused 0 (delta 0), pack-reused 15967\u001b[K\n",
            "Receiving objects: 100% (15967/15967), 14.62 MiB | 25.13 MiB/s, done.\n",
            "Resolving deltas: 100% (10966/10966), done.\n",
            "\u001b[31mERROR: Could not open requirements file: [Errno 2] No such file or directory: 'requirements.txt'\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output shape of the model will be (batch_size, num_anchors, grid_size, grid_size, 5), where 5 refers to the number of predicted values for each anchor box, which includes the bounding box coordinates (x, y, width, height), and objectness score."
      ],
      "metadata": {
        "id": "BAPpHIOxNy9O"
      },
      "id": "BAPpHIOxNy9O"
    },
    {
      "cell_type": "code",
      "source": [
        "# Model\n",
        "model = torch.hub.load(\"ultralytics/yolov5\", \"yolov5s\")  # or yolov5n - yolov5x6, custom"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4kbsK-TNO08l",
        "outputId": "d4b1b89f-314c-4f1e-c6ef-7b2a82b05a88"
      },
      "id": "4kbsK-TNO08l",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/ultralytics_yolov5_master\n",
            "YOLOv5 🚀 2023-9-15 Python-3.10.12 torch-2.0.1+cu118 CPU\n",
            "\n",
            "Fusing layers... \n",
            "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients\n",
            "Adding AutoShape... \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import utils\n",
        "display = utils.notebook_init()  # checks"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5gqYdqqDfOrI",
        "outputId": "efe2153f-40c2-44a8-877c-39c084c22801"
      },
      "id": "5gqYdqqDfOrI",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "YOLOv5 🚀 2023-9-15 Python-3.10.12 torch-2.0.1+cu118 CPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup complete ✅ (2 CPUs, 12.7 GB RAM, 26.3/107.7 GB disk)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train Model"
      ],
      "metadata": {
        "id": "rDYc562d-ZgI"
      },
      "id": "rDYc562d-ZgI"
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Yolo-Doce-YML](https://docs.ultralytics.com/yolov5/tutorials/train_custom_data/#1-create-dataset)\n",
        "\n",
        "[Yolo-PyTorch](https://pytorch.org/hub/ultralytics_yolov5/)"
      ],
      "metadata": {
        "id": "SMd8xBBLwvyG"
      },
      "id": "SMd8xBBLwvyG"
    },
    {
      "cell_type": "code",
      "source": [
        "!cd yolov5 && python train.py --img 512 --batch 16 --epochs 10 --data dataset.yml --weights yolov5s.pt --workers 2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ktZO3doIpqK9",
        "outputId": "0398fd1f-7407-45bb-a396-fdd313c3a90f"
      },
      "id": "ktZO3doIpqK9",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mweights=yolov5s.pt, cfg=, data=dataset.yml, hyp=data/hyps/hyp.scratch-low.yaml, epochs=10, batch_size=16, imgsz=512, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, bucket=, cache=None, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=2, project=runs/train, name=exp, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=0, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\n",
            "\u001b[34m\u001b[1mgithub: \u001b[0mup to date with https://github.com/ultralytics/yolov5 ✅\n",
            "YOLOv5 🚀 v7.0-218-g9e97ac3 Python-3.10.12 torch-2.0.1+cu118 CPU\n",
            "\n",
            "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n",
            "\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 🚀 runs in Comet\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\n",
            "Overriding model.yaml nc=80 with nc=1\n",
            "\n",
            "                 from  n    params  module                                  arguments                     \n",
            "  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n",
            "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
            "  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n",
            "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
            "  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n",
            "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
            "  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n",
            "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
            "  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n",
            "  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n",
            " 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
            " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
            " 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n",
            " 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
            " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
            " 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
            " 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
            " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
            " 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
            " 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
            " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
            " 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
            " 24      [17, 20, 23]  1     16182  models.yolo.Detect                      [1, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n",
            "Model summary: 214 layers, 7022326 parameters, 7022326 gradients, 15.9 GFLOPs\n",
            "\n",
            "Transferred 343/349 items from yolov5s.pt\n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.0005), 60 bias\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/drive/MyDrive/Datasets/Computer_Vision/LPR_Hackthon/LPR_Training_Data/labels.cache... 367 images, 0 backgrounds, 0 corrupt: 100% 367/367 [00:00<?, ?it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/Datasets/Computer_Vision/LPR_Hackthon/LPR_Training_Data/labels.cache... 367 images, 0 backgrounds, 0 corrupt: 100% 367/367 [00:00<?, ?it/s]\n",
            "\n",
            "\u001b[34m\u001b[1mAutoAnchor: \u001b[0m5.02 anchors/target, 1.000 Best Possible Recall (BPR). Current anchors are a good fit to dataset ✅\n",
            "Plotting labels to runs/train/exp3/labels.jpg... \n",
            "Image sizes 512 train, 512 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1mruns/train/exp3\u001b[0m\n",
            "Starting training for 10 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "        0/9         0G     0.1149    0.02273          0         31        512: 100% 23/23 [03:46<00:00,  9.83s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95:  83% 10/12 [01:03<00:12,  6.25s/it]WARNING ⚠️ NMS time limit 2.100s exceeded\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 12/12 [01:16<00:00,  6.33s/it]\n",
            "                   all        367        442    0.00411     0.0204    0.00167   0.000391\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "        1/9         0G    0.09739     0.0221          0         34        512: 100% 23/23 [03:50<00:00, 10.03s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 12/12 [01:09<00:00,  5.80s/it]\n",
            "                   all        367        442     0.0441      0.102     0.0192     0.0042\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "        2/9         0G    0.08341     0.0206          0         36        512: 100% 23/23 [03:46<00:00,  9.84s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 12/12 [01:02<00:00,  5.17s/it]\n",
            "                   all        367        442      0.194      0.086     0.0721     0.0195\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "        3/9         0G    0.07473    0.01991          0         35        512: 100% 23/23 [03:44<00:00,  9.75s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 12/12 [00:58<00:00,  4.90s/it]\n",
            "                   all        367        442      0.402      0.299      0.276      0.076\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "        4/9         0G      0.067    0.01945          0         32        512: 100% 23/23 [03:40<00:00,  9.57s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 12/12 [00:58<00:00,  4.85s/it]\n",
            "                   all        367        442      0.451      0.288      0.303      0.104\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "        5/9         0G    0.06079    0.01978          0         37        512: 100% 23/23 [03:42<00:00,  9.68s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 12/12 [00:58<00:00,  4.90s/it]\n",
            "                   all        367        442      0.523      0.484      0.494      0.193\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "        6/9         0G    0.05521    0.01736          0         29        512: 100% 23/23 [03:40<00:00,  9.59s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 12/12 [00:57<00:00,  4.80s/it]\n",
            "                   all        367        442      0.647      0.557      0.579      0.193\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "        7/9         0G     0.0517    0.01747          0         39        512: 100% 23/23 [03:37<00:00,  9.45s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 12/12 [00:57<00:00,  4.81s/it]\n",
            "                   all        367        442      0.738      0.561      0.666      0.281\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "        8/9         0G    0.04719    0.01559          0         36        512: 100% 23/23 [03:38<00:00,  9.50s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 12/12 [00:57<00:00,  4.79s/it]\n",
            "                   all        367        442      0.775      0.654      0.731      0.334\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "        9/9         0G    0.04346    0.01478          0         38        512: 100% 23/23 [03:37<00:00,  9.44s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 12/12 [00:54<00:00,  4.56s/it]\n",
            "                   all        367        442      0.808      0.679      0.773      0.335\n",
            "\n",
            "10 epochs completed in 0.789 hours.\n",
            "Optimizer stripped from runs/train/exp3/weights/last.pt, 14.3MB\n",
            "Optimizer stripped from runs/train/exp3/weights/best.pt, 14.3MB\n",
            "\n",
            "Validating runs/train/exp3/weights/best.pt...\n",
            "Fusing layers... \n",
            "Model summary: 157 layers, 7012822 parameters, 0 gradients, 15.8 GFLOPs\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 12/12 [00:55<00:00,  4.61s/it]\n",
            "                   all        367        442      0.808      0.679      0.773      0.334\n",
            "Results saved to \u001b[1mruns/train/exp3\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "# replace 'filename' with the name of the file that you want to download\n",
        "files.download('/content/yolov5/runs')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "oM2HrbNI9c6o",
        "outputId": "e5b4c84c-bc3c-477b-9297-2afa31689045"
      },
      "id": "oM2HrbNI9c6o",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_4f51440f-6833-40b0-9e8f-b267a230ca6c\", \"runs\", 4096)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "# replace 'filename' with the name of the file that you want to download\n",
        "files.download('/content/yolov5/runs/train/exp3')\n"
      ],
      "metadata": {
        "id": "9nDlaJND_VGq",
        "outputId": "745096c4-da70-477d-9632-615a659512eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "id": "9nDlaJND_VGq",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_5011b006-adeb-4166-8b1f-d793e793f0c4\", \"exp3\", 4096)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Weights"
      ],
      "metadata": {
        "id": "T68LUQSyB1hE"
      },
      "id": "T68LUQSyB1hE"
    },
    {
      "cell_type": "code",
      "source": [
        "model = torch.hub.load('ultralytics/yolov5', 'custom', path='yolov5/runs/train/exp15/weights/last.pt', force_reload=True)"
      ],
      "metadata": {
        "id": "RL1ckkPUB5vf"
      },
      "execution_count": null,
      "outputs": [],
      "id": "RL1ckkPUB5vf"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}